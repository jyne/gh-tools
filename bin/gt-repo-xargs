#!/usr/bin/env python
import argparse
import json
import logging
import os
import signal
import sys
from subprocess import check_output, CalledProcessError

from ghtools.WorkList import WorkList

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())

def signal_handler(signal, frame):
    os._exit(0)
signal.signal(signal.SIGINT, signal_handler)

def run_analysis_tool(command, repo):
    result = check_output([command, repo['working_directory']])
    return result.splitlines()[-1]

def add_analysis_data(repo, data):
    try:
        o = json.loads(data)
    except ValueError:
        o = data
    repo['analysis_data'] = o

def process_script_arguments():
    parser = argparse.ArgumentParser(description='Analyze GitHub repositories') 
    parser.add_argument('-i', '--infile', type=argparse.FileType('r'),
        help='read list of repositories from this JSON file')
    parser.add_argument('command',
        help='command to execute for each repository')
    args = parser.parse_args()

    return args

if __name__ == "__main__":
    args = process_script_arguments()
    if args.infile is not None:
        wl = WorkList(args.infile)
    else:
        wl = WorkList()

    for repo in wl:
        if not 'working_directory' in repo:
            logger.error('No working directory found for system' + 
                repo['name'])
        try:
            analysis_data = run_analysis_tool(args.command, repo)
            add_analysis_data(repo, analysis_data)
        except CalledProcessError as e:
            logger.error('Command failed, return code: ' + e.returncode)
            logger.error('Output:\n' + e.output)
            repo.set_status_failed()

    wl.write(sys.stdout)
